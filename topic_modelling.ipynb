{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modelling using BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nbformat in /Users/sophie/opt/anaconda3/lib/python3.9/site-packages (5.5.0)\n",
      "Collecting nbformat\n",
      "  Using cached nbformat-5.9.2-py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: traitlets>=5.1 in /Users/sophie/opt/anaconda3/lib/python3.9/site-packages (from nbformat) (5.1.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /Users/sophie/opt/anaconda3/lib/python3.9/site-packages (from nbformat) (4.16.0)\n",
      "Requirement already satisfied: jupyter-core in /Users/sophie/opt/anaconda3/lib/python3.9/site-packages (from nbformat) (4.11.1)\n",
      "Requirement already satisfied: fastjsonschema in /Users/sophie/opt/anaconda3/lib/python3.9/site-packages (from nbformat) (2.16.2)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /Users/sophie/opt/anaconda3/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/sophie/opt/anaconda3/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat) (21.4.0)\n",
      "Installing collected packages: nbformat\n",
      "  Attempting uninstall: nbformat\n",
      "    Found existing installation: nbformat 5.5.0\n",
      "    Uninstalling nbformat-5.5.0:\n",
      "      Successfully uninstalled nbformat-5.5.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda-repo-cli 1.0.27 requires clyent==1.2.1, but you have clyent 1.2.2 which is incompatible.\n",
      "conda-repo-cli 1.0.27 requires nbformat==5.4.0, but you have nbformat 5.9.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nbformat-5.9.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade nbformat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries/data required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "from bertopic import BERTopic\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18520, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>date</th>\n",
       "      <th>location_article</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The article discusses the passing of the new C...</td>\n",
       "      <td>2011-07-07</td>\n",
       "      <td>Juba</td>\n",
       "      <td>4.859363</td>\n",
       "      <td>31.571250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The article discusses the military actions tak...</td>\n",
       "      <td>2011-07-03</td>\n",
       "      <td>Abyei</td>\n",
       "      <td>9.838551</td>\n",
       "      <td>28.486396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The article discusses the signing of a Framewo...</td>\n",
       "      <td>2011-06-30</td>\n",
       "      <td>Southern Kordofan</td>\n",
       "      <td>11.036544</td>\n",
       "      <td>30.895824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The article discusses the upcoming independenc...</td>\n",
       "      <td>2011-07-04</td>\n",
       "      <td>South Sudan</td>\n",
       "      <td>6.876992</td>\n",
       "      <td>31.306979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The article discusses the need for South Sudan...</td>\n",
       "      <td>2011-07-02</td>\n",
       "      <td>Juba</td>\n",
       "      <td>4.859363</td>\n",
       "      <td>31.571250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             summary       date  \\\n",
       "0  The article discusses the passing of the new C... 2011-07-07   \n",
       "1  The article discusses the military actions tak... 2011-07-03   \n",
       "2  The article discusses the signing of a Framewo... 2011-06-30   \n",
       "3  The article discusses the upcoming independenc... 2011-07-04   \n",
       "4  The article discusses the need for South Sudan... 2011-07-02   \n",
       "\n",
       "    location_article        lat        lng  \n",
       "0               Juba   4.859363  31.571250  \n",
       "1              Abyei   9.838551  28.486396  \n",
       "2  Southern Kordofan  11.036544  30.895824  \n",
       "3        South Sudan   6.876992  31.306979  \n",
       "4               Juba   4.859363  31.571250  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data and perform preprocessing\n",
    "\n",
    "df = pd.read_csv(\"data/full_dataset_with_sentiment_combined2.csv\", parse_dates=[\"date\"]) # Read data into 'df' dataframe\n",
    "print(df.shape) # Print dataframe shape\n",
    "\n",
    "docs = df[\"summary\"].tolist() # Create a list containing all article summaries\n",
    "\n",
    "df.head() # Show first 5 dataframe entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting BERTopic\n",
    "\n",
    "This might take a while on a CPU. In the background a pre-trained Large Language Model, called the sentence embedder, is used to convert the articles to a semantic vector space. We then perform clustering in this space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/92/hrvf3fy95hlf485k7tfskn100000gn/T/ipykernel_24860/745638604.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'southsudan_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mbertopic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBERTopic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'southsudan_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbertopic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBERTopic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcalculate_probabilities\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Initialize the BERTopic model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/bertopic/_bertopic.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, path, embedding_model)\u001b[0m\n\u001b[1;32m   2996\u001b[0m                     \u001b[0mtopic_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2997\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2998\u001b[0;31m                     \u001b[0mtopic_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2999\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtopic_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_read_fileobject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36m_unpickle\u001b[0;34m(fobj, filename, mmap_mode)\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m             warnings.warn(\"The file '%s' has been generated with a \"\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1210\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1212\u001b[0;31m                 \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1213\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0m_Stop\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "if os.path.exists('southsudan_model'):\n",
    "    bertopic = BERTopic.load('southsudan_model')\n",
    "else:\n",
    "    bertopic = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True) # Initialize the BERTopic model\n",
    "\n",
    "    bertopic.fit_transform(docs) # Fit the model to the list of article summaries\n",
    "    bertopic.save(\"southsudan_model\") # Save the trained model as \"southsudan_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Due to the modularity of the model, there is a lot of randomness that hinders reproducibiity of the model.\n",
    "#To fight this, you can for example set random state in the dimensionality reduction step via the following lines \n",
    "#or explore a different approach\n",
    "\n",
    "#from bertopic import BERTopic\n",
    "#from umap import UMAP\n",
    "\n",
    "#umap_model = UMAP(n_neighbors=15, n_components=5, \n",
    "#                  min_dist=0.0, metric='cosine', random_state=42)\n",
    "#topic_model = BERTopic(umap_model=umap_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive visualization of the vector space\n",
    "\n",
    "As you can see, documents with related topics are close in the space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertopic.visualize_documents(docs) # Create a plot of the topics, this may take a while"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating smaller topics\n",
    "\n",
    "Within our list of topics, we find topics that are semantically closest to 4 keywords:\n",
    "\n",
    "\"Hunger\", \"Refugees\", \"Conflict\", and \"Humanitarian\".\n",
    "\n",
    "**Feel free to change this approach!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a function to calculate a list of the top n topics related to (a) given keyword(s)\n",
    "\n",
    "def get_relevant_topics(bertopic_model, keywords, top_n):\n",
    "    '''\n",
    "    Retrieve a list of the top n number of relevant topics to the provided (list of) keyword(s)\n",
    "    \n",
    "    \n",
    "    Parameters:\n",
    "        bertopic_model: a (fitted) BERTopic model object\n",
    "        \n",
    "        keywords:   a string containing one or multiple keywords to match against,\n",
    "                    \n",
    "                    This can also be a list in the form of ['keyword(s)', keyword(s), ...]\n",
    "                    \n",
    "                    In this case a maximum of top_n topics will be found per list element \n",
    "                    and subsetted to the top_n most relevant topics.\n",
    "                    \n",
    "                    !!!\n",
    "                    Take care that this method only considers the relevancy per inputted keyword(s) \n",
    "                    and not the relevancy to the combined list of keywords.\n",
    "                    \n",
    "                    In other words, topics that appear in the output might be significantly related to a \n",
    "                    particular element in the list of keywords but not so to any other element, \n",
    "                    \n",
    "                    while topics that do not appear in the output might be significantly related to the \n",
    "                    combined list of keywords but not much to any of the keyword(s) in particular.\n",
    "                    !!!\n",
    "                    \n",
    "        top_n: an integer indicating the number of desired relevant topics to be retrieved\n",
    "        \n",
    "        \n",
    "        Return: a list of the top_n (or less) topics most relevant to the (list of) provided keyword(s)\n",
    "    '''\n",
    "    \n",
    "    if type(keywords) is str: keywords = [keywords] # If a single string is provided convert it to list type\n",
    "    \n",
    "    relevant_topics = list() # Initilize an empty list of relevant topics\n",
    "    \n",
    "    for keyword in keywords: # Iterate through list of keywords\n",
    "        \n",
    "        # Find the top n number of topics related to the current keyword(s)\n",
    "        topics = bertopic_model.find_topics(keyword, top_n = top_n)\n",
    "        \n",
    "        # Add the topics to the list of relevant topics in the form of (topic_id, relevancy)\n",
    "        relevant_topics.extend(\n",
    "            zip(topics[0], topics[1]) # topics[0] = topic_id, topics[1] = relevancy\n",
    "        )\n",
    "    \n",
    "    \n",
    "    relevant_topics.sort(key=lambda x: x[1]) # Sort the list of topics on ASCENDING ORDER of relevancy\n",
    "    \n",
    "    # Get a list of the set of unique topics (with greates relevancy in case of duplicate topics)\n",
    "    relevant_topics = list(dict(relevant_topics).items())\n",
    "    \n",
    "    \n",
    "    relevant_topics.sort(key=lambda x: x[1], reverse=True) # Now sort the list of topics on DESCENDING ORDER of relevancy\n",
    "    \n",
    "    return relevant_topics[:10] # Return a list of the top_n unique relevant topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 topics related to the keywords 'hunger' and 'food insecurity'\n",
    "relevant_topics = get_relevant_topics(bertopic_model = bertopic, keywords=['hunger', 'food insecurity'], top_n=10)\n",
    "\n",
    "topic_ids = [el[0] for el in relevant_topics] # Create seperate list of topic IDs\n",
    "\n",
    "for topic_id, relevancy in relevant_topics: # Print neat list of (topic_id, relevancy) tuples\n",
    "    print(topic_id, relevancy)\n",
    "    \n",
    "df[\"hunger\"] = [t in topic_ids for t in bertopic.topics_] # Add boolean column to df if topic in list of relevant topics\n",
    "\n",
    "# View the Count, Name, Representation, and Representative Docs for the relevant topics\n",
    "bertopic.get_topic_info().set_index('Topic').loc[topic_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 topics related to the keywords 'refugees' and 'displaced'\n",
    "relevant_topics = get_relevant_topics(bertopic_model = bertopic, keywords=['refugees', 'displaced'], top_n=10)\n",
    "\n",
    "topic_ids = [el[0] for el in relevant_topics] # Create seperate list of topic IDs\n",
    "\n",
    "for topic_id, relevancy in relevant_topics: # Print neat list of (topic_id, relevancy) tuples\n",
    "    print(topic_id, relevancy)\n",
    "    \n",
    "df[\"refugees\"] = [t in topic_ids for t in bertopic.topics_] # Add boolean column to df if topic in list of relevant topics\n",
    "\n",
    "# View the Count, Name, Representation, and Representative Docs for the relevant topics\n",
    "bertopic.get_topic_info().set_index('Topic').loc[topic_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 topics related to the keyword 'humanitarian'\n",
    "relevant_topics = get_relevant_topics(bertopic_model = bertopic, keywords=['humanitarian'], top_n=10)\n",
    "\n",
    "topic_ids = [el[0] for el in relevant_topics] # Create seperate list of topic IDs\n",
    "\n",
    "for topic_id, relevancy in relevant_topics: # Print neat list of (topic_id, relevancy) tuples\n",
    "    print(topic_id, relevancy)\n",
    "    \n",
    "df[\"humanitarian\"] = [t in topic_ids for t in bertopic.topics_] # Add boolean column to df if topic in list of relevant topics\n",
    "\n",
    "# View the Count, Name, Representation, and Representative Docs for the relevant topics\n",
    "bertopic.get_topic_info().set_index('Topic').loc[topic_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 topics related to the keywords 'conflict', 'fighting', and 'murder'\n",
    "relevant_topics = get_relevant_topics(bertopic_model = bertopic, keywords=['conflict', 'fighting', 'murder'], top_n=10)\n",
    "\n",
    "topic_ids = [el[0] for el in relevant_topics] # Create seperate list of topic IDs\n",
    "\n",
    "for topic_id, relevancy in relevant_topics: # Print neat list of (topic_id, relevancy) tuples\n",
    "    print(topic_id, relevancy)\n",
    "    \n",
    "df[\"conflict\"] = [t in topic_ids for t in bertopic.topics_] # Add boolean column to df if topic in list of relevant topics\n",
    "\n",
    "# View the Count, Name, Representation, and Representative Docs for the relevant topics\n",
    "bertopic.get_topic_info().set_index('Topic').loc[topic_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = pd.read_csv(\"data/articles_summary_cleaned.csv\", parse_dates=[\"date\"])\n",
    "\n",
    "# Combine article summaries with the newly created features\n",
    "df = original_df.merge(\n",
    "    df[[\"summary\", \"hunger\", \"refugees\", \"humanitarian\", \"conflict\"]],\n",
    "    how=\"left\",\n",
    "    left_on=\"summary\",\n",
    "    right_on=\"summary\",\n",
    ")\n",
    "\n",
    "df.to_csv(\"data/articles_topics.csv\", index=False) # Save DataFrame to articles_topics.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df))\n",
    "print(len(df[(df[\"hunger\"]==False) & (df[\"refugees\"] == False) & (df[\"humanitarian\"] == False) & (df[\"conflict\"] == False)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of articles that do not get sorted into either of the categories. So, feel free to change or expand this approach!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
